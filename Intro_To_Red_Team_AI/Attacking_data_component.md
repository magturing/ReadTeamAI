
Attacking Data Components

The data component comprises everything related to the data the model operates on, including training data and inference data. As ML models are inherently data-dependent, the data is a good starting point for adversaries to attack ML-based systems. Both model quality and usability rely highly on the type of data the model was trained on. Therefore, even minor disruptions or manipulations to the data component can have vast consequences for the final model. Additionally, depending on the type of data a model operates on, a data leak can lead to legal consequences for the victim â€” for instance, GDPR-related consequences for data containing personally identifiable information (PII).
Risks

The quality of an ML model depends on the quality of the data it was trained on. One of the most significant risks of systems using generative AI is improper training data. This can include biases in the training data and unrepresentative training data that does not match the kind of data the fully trained model is queried on. Issues in the training data set may lead to low-quality results generated by the fully trained model and discriminatory or harmful output. Therefore, creating proper training data is of utmost significance for the quality of the ML-based system.

Data poisoning is an attack vector with a similar impact to model poisoning discussed in the previous section. The main difference is that adversaries do not manipulate the model parameters directly but instead manipulate the training data during the training process. Adversaries manipulating training data in generative AI models pose a significant threat to the integrity and reliability of these systems. Generative AI models, such as those used for text generation, image synthesis, or deepfake creation, rely on high-quality training data to learn patterns and generate realistic outputs. Attackers who introduce malicious or biased data into the training set can subtly influence the model's behavior. This type of data poisoning attack may have a similar impact to model poisoning, including:

    generation of misleading output
    generation of biased output
    generation of harmful content

In some cases, attackers may embed specific triggers in the data, leading the model to produce erroneous or adversarial outputs when prompted with specific inputs. This is known as a backdoor attack. Such manipulations can degrade the quality of the model, reduce trust in its outputs, or exploit it for malicious purposes like misinformation campaigns.

Diagram comparing Normal Learning with a clean dataset and Poisoning Attack with a compromised dataset affecting the machine learning model.

Due to the large amounts of data required to train and operate an ML model, there is an inherent risk of data leaks and unauthorized leakage of potentially sensitive data. Adversaries may exploit security vulnerabilities, resulting in unauthorized access to training data to obtain access to training data or inference data. Depending on what information is contained within these data sets, sensitive user data may be leaked to adversaries. This can lead to financial and legal repercussions for the system operator and even open the door to further attack vectors, such as a complete system takeover. In some cases, stolen data may contain unique or curated datasets that took years to assemble, making it a valuable asset for competitors or malicious actors. Attackers can reverse-engineer the generative AI model using the stolen data or use it to create adversarial attacks that target the original model.

Moreover, by obtaining insights into the dataset, adversaries can craft specific inputs to manipulate the model's outputs or exploit its vulnerabilities. This makes securing training data as critical as protecting the model itself, as the loss of this data can lead to both direct financial harm and long-term damage to trust and innovation in AI-driven systems.
Tactics, Techniques, and Procedures (TTPs)

As discussed above, the quality of any ML model is highly dependent on the quality of the training data set. As such, the consequences of adversaries manipulating training data in generative AI models extend beyond compromised performance. Introducing biased or false data could have severe ethical, legal, or safety implications in sensitive applications such as:

    content creation
    legal document generation
    AI-based healthcare advice

Therefore, training data manipulation is a lucrative attack vector for adversaries. However, an often challenging requirement for these attacks is knowing which data a model is trained on and injecting malicious data into the training data set. Depending on where the training data is fetched from, how it is sanitized or validated, and the exact setup of the training process, this requirement might be impossible to overcome. In other scenarios, manipulation of the training data or process is possible. For example, in federated learning systems, where multiple parties contribute to training, adversaries can inject poisoned updates during their participation, skewing the global model without raising suspicion.

Furthermore, adversaries interested in stealing training data from an ML-based system may use a mix of traditional and novel TTPs. These include identifying and exploiting weak security practices regarding data storage and transmission within the target organization. For instance:

    poorly configured cloud storage
    insufficient encryption at rest or at transit
    insecure data pipelines
    usage of vulnerable APIs

Additionally, adversaries may exploit the same vulnerabilities and misconfigurations in third-party vendors or data providers supplying or curating training data for generative AI models. Compromising a vendor in the ML model's supply chain allows attackers to access the dataset before it reaches the organization (Supply Chain Attacks).

Lastly, employees and contractors with legitimate access to sensitive data pose an insider threat. They may be exploited via traditional attack vectors such as phishing or social engineering to compromise credentials and obtain unauthorized access to data. On top of that, insider threats may deliberately exfiltrate training data for personal financial gain, industrial espionage, or other personal reasons. Since employees may have authorized access, they can steal data with little need for advanced hacking techniques or the presence of security vulnerabilities., making this threat harder to detect.
