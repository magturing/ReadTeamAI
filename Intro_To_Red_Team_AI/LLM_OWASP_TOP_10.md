1. Prompt injection :- Attackers manipulate the LLM's input directly or indirectly to cause malicious or illegal behavior.
    Prompt injection is a type of security vulnerability that occurs when an attacker can manipulate an LLM's input, potentially causing the LLM to deviate from its intended behavior. While this can include seemingly benign examples, such as tricking an LLM tech-support chatbot into providing cooking recipes, it can also lead to LLMs generating deliberately false information, hate speech, or other harmful or illegal content. Furthermore, prompt injection attacks may be used to obtain sensitive information in case such information has been shared with the LLM

2. Insecure Output Handling: LLM Output is handled insecurely, resulting in injection vulnerabilities such as Cross-Site Scripting (XSS), SQL Injection, or Command Injection.

LLM-generated text should be treated the same as untrusted user input. If web applications do not validate or sanitize LLM output properly, common web vulnerabilities such as Cross-Site Scripting (XSS), SQL injection, or code injection may arise.

    Furthermore, LLM output should always be checked to see if it matches the expected syntax and values. For instance, we can imagine a scenario where an LLM queries data from the database based on user-provided text and displays the content to the user. If the user supplies input like Give me the content of blog post #3, the model might generate the output SELECT content FROM blog WHERE id=3. The backend web application can then use the LLM output to query the database and display the corresponding content to the user. Apart from the potential SQL injection attack vector, applying some plausibility checks to the LLM-generated SQL query is crucial. Without this kind of checks, unintended behavior might occur. All data is lost if an attacker can get the LLM to generate the query DROP TABLE blog.

3. Training Data Poisoning: Attackers inject malicious or misleading data into the LLM's training data, compromising performance or creating backdoors.
    The quality and capabilities of any LLM depend highly on the training data used in the training process. Training Data Poisoning is the manipulation of all or some training data to introduce biases that skew the model into making intentionally bad decisions. Depending on the purpose of the poisoned LLM, this can result in a damaged reputation or even more severe security vulnerabilities in software components if code snippets generated by the LLM are used elsewhere.

    To successfully perform training data poisoning, an attacker must obtain access to the training data on which the LLM is trained. If an LLM is trained on publicly available data, sanitizing the training data is essential to verify its integrity and remove any unwanted biases. Further mitigation strategies include fine-granular verification checks on the supply chain of the training data, the legitimacy of the training data, and proper input filters that remove false or erroneous training data.

4. Model Denial of Service: Attackers feed inputs to the LLM that result in high resource consumption, potentially causing disruptions to the LLM service.

    A denial-of-service (DoS) attack on an LLM is similar to a DoS attack on any other system. The goal of a DoS attack is to impair other user's ability to use the LLM by decreasing the service's availability. Since LLMs are typically computationally expensive, a specifically crafted query that results in high resource consumption can easily overwhelm available system resources, resulting in a system outage if the service has not been set up with proper safeguards or sufficient resources.

    To prevent DoS attacks, proper validation of user input is essential. However, due to the indeterministic and unpredictable nature of LLMs, it is impossible to prevent DoS attacks by simply blacklisting specific user queries. Therefore, this countermeasure needs to be complemented by strict rate limits and resource consumption monitoring to enable early detection of potential DoS attacks.

5. Supply Chain Vulnerabilities: Attackers exploit vulnerabilities in any part of the LLM supply chain.

    Supply chain vulnerabilities regarding LLMs cover any systems or software in the LLM supply chain. This can include the training data (refer to LLM03), pre-trained LLMs from another provider, and even plugins (cf. LLM07) or other systems interacting with the LLM.

    The impact of supply chain vulnerabilities varies greatly. A typical example is a data leak or disclosure of intellectual property.

6. Sensitive Information Disclosure: Attackers trick the LLM into revealing sensitive information in the response.

    LLMs may inadvertently reveal confidential data in their responses. This can result in unauthorized data access, privacy violations, and even security breaches. Limiting the amount and type of information an LLM can access is essential. In particular, if an LLM operates on sensitive or business-critical information such as customer data, access to query the LLM should be adequately restricted to minimize the risk of data leaks. If an LLM is fine-tuned using a custom training data set, it is crucial to remember that it might be tricked into revealing details about the training data. As such, sensitive information contained in the training data should be identified and assessed according to its criticality.

    Furthermore, sensitive information provided to the LLM in an input prompt may be revealed through prompt injection attack payloads (cf. LLM01), even if the LLM is told to keep the data secret.

7.  Insecure Plugin Design: Attackers exploit security vulnerabilities in LLM plugins.

    LLMs can be integrated with other systems via plugins. If such a plugin blindly trusts output from the LLM without any sanitization or validation, security vulnerabilities may arise. Depending on the concrete functionality of the plugin, common web vulnerabilities such as Cross-Site Scripting (XSS), SQL Injection, Server-side Request Fraud (SSRF), and Remote Code Execution can occur.

8. Excessive Agency: Attackers exploit insufficiently restricted LLM access.

    Security vulnerabilities may arise if an LLM is given more agency than is required for its operation. Similar to the principle of least privilege, it is vital to restrict an LLM's capabilities as much as possible to reduce the attack surface for malicious actors.

    For instance, if an LLM can interface with other systems or services, we need to ensure that a whitelisting is implemented to enable the LLM to access only the required services. On top of that, we need to think about what we want the LLM to do and restrict the LLM's permission to that specific purpose. Consider a scenario where an LLM interfaces with a SQL database to fetch data for the user. If we don't restrict the LLM's database access, it might be possible to trick it into executing DELETE or INSERT statements, affecting the integrity of the database.

9.  Overreliance: An organization is overly reliant on an LLM's output for critical business decisions, potentially leading to security issues from unexpected LLM behavior.

    Due to the way LLMs work, they are inherently prone to providing false information. This can include factually incorrect statements but also erroneous or buggy code snippets. If an LLM is integrated into an organization's business processes without proper validation and checks of LLM-provided information, security vulnerabilities can arise from incorrect data provided by the LLM. As such, it is crucial to manually check and verify the information the LLM provides before using it in sensitive operations.

10. Model Theft: Attackers gain unauthorized access to the LLM itself, stealing intellectual property and potentially causing financial harm.

    Model theft occurs when an attacker is able to steal the LLM itself, i.e., its weights and parameters. Afterward, an attacker would be able to replicate the LLM in its entirety. This could damage the victim's reputation or enable an attacker to offer the same service at a cheaper rate since the attacker does not have the significant sunk costs of the resource and time-intensive training process that the victim went through to train the LLM.

    To mitigate model theft, proper authentication and access control mechanisms are vital, as they prevent unauthorized access to the LLM.